{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.Requirement already satisfied: datasets in c:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (2.18.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (3.8.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (1.26.1)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (15.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in c:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (1.3.5)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (2.26.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (4.62.3)\n",
      "Requirement already satisfied: xxhash in c:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets) (2024.2.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (3.8.4)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in c:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (0.20.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets) (22.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets) (2.0.9)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets) (1.8.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface-hub>=0.19.4->datasets) (4.10.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from packaging->datasets) (3.0.6)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests>=2.19.0->datasets) (3.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas->datasets) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: colab 1.13.5 has a non-standard dependency specifier pytz>=2011n. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of colab or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\n",
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "from datasets import Dataset,DatasetDict, load_dataset\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 30476/30476 [00:00<00:00, 733803.35 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 8127/8127 [00:00<00:00, 677191.45 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 16545/16545 [00:00<00:00, 870808.88 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['review_text', 'F', 'BR', 'AU', 'FI', 'IR', 'A', 'L', 'LF', 'MN', 'O', 'PE', 'SC', 'SE', 'US', 'PO', '__index_level_0__'],\n",
       "        num_rows: 30476\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['review_text', 'F', 'BR', 'AU', 'FI', 'IR', 'A', 'L', 'LF', 'MN', 'O', 'PE', 'SC', 'SE', 'US', 'PO', '__index_level_0__'],\n",
       "        num_rows: 8127\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['review_text', 'F', 'BR', 'AU', 'FI', 'IR', 'A', 'L', 'LF', 'MN', 'O', 'PE', 'SC', 'SE', 'US', 'PO', '__index_level_0__'],\n",
       "        num_rows: 16545\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_directory = '../_2_data_generation-gemini-pro/Dataset/'\n",
    "files = os.listdir(dataset_directory)\n",
    "\n",
    "data_df = pd.DataFrame()\n",
    "\n",
    "for file in files:\n",
    "    df = pd.read_csv(f\"{dataset_directory}{file}\")\n",
    "    data_df = pd.concat([data_df, df], axis=0)\n",
    "\n",
    "bool_columns = ['F', 'BR', 'AU', 'FI', 'IR', 'A', 'L', 'LF', 'MN', 'O', 'PE', 'SC', 'SE', 'US', 'PO']\n",
    "data_df[bool_columns] = data_df[bool_columns].astype(bool)\n",
    "data_df = data_df.dropna(subset=['review_text'])\n",
    "\n",
    "train_ratio = 0.62\n",
    "val_ratio = 0.08\n",
    "test_ratio = 0.30\n",
    "\n",
    "train_dataset, test_dataset = train_test_split(data_df, test_size=test_ratio, random_state=42)\n",
    "\n",
    "train_dataset, val_dataset = train_test_split(train_dataset, test_size=val_ratio/(1-train_ratio), random_state=42)\n",
    "\n",
    "\n",
    "split_datasets = DatasetDict({\n",
    "    'train': Dataset.from_pandas(train_dataset),\n",
    "    'validation': Dataset.from_pandas(val_dataset),\n",
    "    'test': Dataset.from_pandas(test_dataset)\n",
    "})\n",
    "\n",
    "split_datasets.save_to_disk(\"./hugging-face-dataset/dataset\")\n",
    "split_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset\n",
    "\n",
    "Note that you can also easily load your local data (i.e. csv files, txt files, Parquet files, JSON, ...) as explained [here](https://huggingface.co/docs/datasets/loading.html#local-and-remote-files).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: review_text, Data Type: string\n",
      "Feature: F, Data Type: bool\n",
      "Feature: BR, Data Type: bool\n",
      "Feature: AU, Data Type: bool\n",
      "Feature: FI, Data Type: bool\n",
      "Feature: IR, Data Type: bool\n",
      "Feature: A, Data Type: bool\n",
      "Feature: L, Data Type: bool\n",
      "Feature: LF, Data Type: bool\n",
      "Feature: MN, Data Type: bool\n",
      "Feature: O, Data Type: bool\n",
      "Feature: PE, Data Type: bool\n",
      "Feature: SC, Data Type: bool\n",
      "Feature: SE, Data Type: bool\n",
      "Feature: US, Data Type: bool\n",
      "Feature: PO, Data Type: bool\n",
      "Feature: __index_level_0__, Data Type: int64\n"
     ]
    }
   ],
   "source": [
    "dataset = DatasetDict.load_from_disk(\"./hugging-face-dataset/dataset/\")\n",
    "features = dataset[\"train\"].features\n",
    "for feature_name, feature_info in features.items():\n",
    "    data_type = feature_info.dtype\n",
    "    print(f\"Feature: {feature_name}, Data Type: {data_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['review_text', 'F', 'BR', 'AU', 'FI', 'IR', 'A', 'L', 'LF', 'MN', 'O', 'PE', 'SC', 'SE', 'US', 'PO', '__index_level_0__'],\n",
       "        num_rows: 30476\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['review_text', 'F', 'BR', 'AU', 'FI', 'IR', 'A', 'L', 'LF', 'MN', 'O', 'PE', 'SC', 'SE', 'US', 'PO', '__index_level_0__'],\n",
       "        num_rows: 8127\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['review_text', 'F', 'BR', 'AU', 'FI', 'IR', 'A', 'L', 'LF', 'MN', 'O', 'PE', 'SC', 'SE', 'US', 'PO', '__index_level_0__'],\n",
       "        num_rows: 16545\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the dataset contains 3 splits: one for training, one for validation and one for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the first example of the training split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'review_text': \"I've had twitter for a very long time and I love it, but since the latest update, I can't open my app anymore. I've closed the app and tried reopening it, I've deleted and reinstalled the app as well, but it won't open. Please fix this\",\n",
       " 'F': False,\n",
       " 'BR': True,\n",
       " 'AU': False,\n",
       " 'FI': False,\n",
       " 'IR': False,\n",
       " 'A': False,\n",
       " 'L': False,\n",
       " 'LF': False,\n",
       " 'MN': False,\n",
       " 'O': False,\n",
       " 'PE': False,\n",
       " 'SC': False,\n",
       " 'SE': False,\n",
       " 'US': False,\n",
       " 'PO': False,\n",
       " '__index_level_0__': 44897}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = dataset['train'][0]\n",
    "example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset consists of app reviews, labeled with one or more emotions.\n",
    "\n",
    "Let's create a list that contains the labels, as well as 2 dictionaries that map labels to integers and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['F',\n",
       " 'BR',\n",
       " 'AU',\n",
       " 'FI',\n",
       " 'IR',\n",
       " 'A',\n",
       " 'L',\n",
       " 'LF',\n",
       " 'MN',\n",
       " 'O',\n",
       " 'PE',\n",
       " 'SC',\n",
       " 'SE',\n",
       " 'US',\n",
       " 'PO']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [label for label in dataset['train'].features.keys() if label not in ['__index_level_0__', 'review_text']]\n",
    "id2label = {idx:label for idx, label in enumerate(labels)}\n",
    "label2id = {label:idx for idx, label in enumerate(labels)}\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data\n",
    "\n",
    "As models like BERT don't expect text as direct input, but rather `input_ids`, etc., we tokenize the text using the tokenizer. Here I'm using the `AutoTokenizer` API, which will automatically load the appropriate tokenizer based on the checkpoint on the hub.\n",
    "\n",
    "What's a bit tricky is that we also need to provide labels to the model. For multi-label text classification, this is a matrix of shape (batch_size, num_labels). Also important: this should be a tensor of floats rather than integers, otherwise PyTorch' `BCEWithLogitsLoss` (which the model will use) will complain, as explained [here](https://discuss.pytorch.org/t/multi-label-binary-classification-result-type-float-cant-be-cast-to-the-desired-output-type-long/117915/3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def preprocess_data(examples):\n",
    "  if examples[\"review_text\"] == None:\n",
    "    return None\n",
    "  # take a batch of texts\n",
    "  text = examples[\"review_text\"]\n",
    "  # encode them\n",
    "  encoding = tokenizer(text, padding=\"max_length\", truncation=True, max_length=128)\n",
    "  # add labels\n",
    "  labels_batch = {k: examples[k] for k in examples.keys() if k in labels}\n",
    "  # create numpy array of shape (batch_size, num_labels)\n",
    "  labels_matrix = np.zeros((len(text), len(labels)))\n",
    "  # fill numpy array\n",
    "  for idx, label in enumerate(labels):\n",
    "    labels_matrix[:, idx] = labels_batch[label]\n",
    "\n",
    "  encoding[\"labels\"] = labels_matrix.tolist()\n",
    "  return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 30476/30476 [00:03<00:00, 10020.83 examples/s]\n",
      "Map: 100%|██████████| 8127/8127 [00:00<00:00, 9151.05 examples/s]\n",
      "Map: 100%|██████████| 16545/16545 [00:01<00:00, 9306.51 examples/s]\n"
     ]
    }
   ],
   "source": [
    "encoded_dataset = dataset.map(preprocess_data, batched=True, remove_columns=dataset[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n"
     ]
    }
   ],
   "source": [
    "example = encoded_dataset['train'][0]\n",
    "print(example.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[CLS] i've had twitter for a very long time and i love it, but since the latest update, i can't open my app anymore. i've closed the app and tried reopening it, i've deleted and reinstalled the app as well, but it won't open. please fix this [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(example['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BR']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[id2label[idx] for idx, label in enumerate(example['labels']) if label == 1.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we set the format of our data to PyTorch tensors. This will turn the training, validation and test sets into standard PyTorch [datasets](https://pytorch.org/docs/stable/data.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dataset['train']['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved as a serialized file: output/serialized_encoded_review_dataset.pkl\n"
     ]
    }
   ],
   "source": [
    "#Saving the DataFrame as in serialized format to preserve the properties of the data structures(Tensor)\n",
    "\n",
    "import pickle\n",
    "\n",
    "#Define the file path where you want to save the serialized DataFrame\n",
    "file_path = 'output/serialized_encoded_review_dataset.pkl'\n",
    "\n",
    "# Step 3: Save the DataFrame using pickle\n",
    "with open(file_path, 'wb') as f:\n",
    "    pickle.dump(encoded_dataset, f)\n",
    "\n",
    "print(\"Dataset saved as a serialized file:\", file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
